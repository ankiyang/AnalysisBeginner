KNN

##
做分类 做回归 的区别: 最后做预测时候的决策方式不同

做分类预测的时候,一般是选择多数表决法.即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。
而KNN做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值.


##

三个最终的要素是:
k值的选取，距离度量的方式和分类决策规则。

对于分类决策规则，一般都是使用前面提到的多数表决法。所以我们重点是关注与k值的选择和距离的度量方式。

##
对于k值的选择，没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的k值。
###
选择较小的k值，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是泛化误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；
选择较大的k值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少泛化误差，但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。

一个极端是k等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单。

###数值归一化

数字差值最大的属性对计算结果的影响最大.
n种特征同等重要:
  其中一种数值较大的特种不应该严重影响到计算结果.
  
  将取值范围处理为0到1或者-1到1之间.
  
  将任意范围的特征值转化为0到1区间内的值:
     newValue=(oldValue/min)/(max-min)
     min:数据集中的最小特征值
     max:数据集中的最大特征值
     
##KNN算法蛮力实现
　　　　从本节起，我们开始讨论KNN算法的实现方式。首先我们看看最想当然的方式。

　　　　既然我们要找到k个最近的邻居来做预测，那么我们只需要计算预测样本和所有训练集中的样本的距离，然后计算出最小的k个距离即可，接着多数表决，很容易做出预测。这个方法的确简单直接，在样本量少，样本特征少的时候有效。但是在实际运用中很多时候用不上，为什么呢？因为我们经常碰到样本的特征数有上千以上，样本量有几十万以上，如果我们这要去预测少量的测试集样本，算法的时间效率很成问题。因此，这个方法我们一般称之为蛮力实现。比较适合于少量样本的简单模型的时候用。

　　　　既然蛮力实现在特征多，样本多的时候很有局限性，那么我们有没有其他的好办法呢？有！这里我们讲解两种办法，一个是KD树实现，一个是球树实现。


##KNN的主要优点有：

　　　　1） 理论成熟，思想简单，既可以用来做分类也可以用来做回归

　　　　2） 可用于非线性分类

　　　　3） 训练时间复杂度比支持向量机之类的算法低，仅为O(n)

　　　　4） 和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感

　　　　5） 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合

　　　　6）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分


##KNN的主要缺点有：

　　　　1）计算量大，尤其是特征数非常多的时候

　　　　2）样本不平衡的时候，对稀有类别的预测准确率低

　　　　3）KD树，球树之类的模型建立需要大量的内存

　　　　4）使用懒散学习方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢

　　　　5）相比决策树模型，KNN模型可解释性不强

